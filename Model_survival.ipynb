{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **This notebook aims to study interpretability methods for differents Survival ML**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alqui\\AppData\\Local\\Temp\\ipykernel_12192\\2036575907.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "c:\\Users\\alqui\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Displays for output\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Standard ML import\n",
    "\n",
    "import sklearn \n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "import optuna\n",
    "import shap\n",
    "import lime\n",
    "from itertools import product\n",
    "from tools import concordance_censored, generate_param_grid, get_param_combinations, count_combinations, generate_random_numbers, random_number_dict\n",
    "\n",
    "\n",
    "# Survival Analysis tools\n",
    "import sksurv\n",
    "#import survlimepy\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis, RandomSurvivalForest\n",
    "from sksurv.svm import FastKernelSurvivalSVM\n",
    "from sksurv.metrics import concordance_index_ipcw, cumulative_dynamic_auc, integrated_brier_score\n",
    "#from survlimepy import SurvLimeExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pipeline for data's preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(dataframe):\n",
    "\n",
    "    # Separate categorical and numerical columns\n",
    "    categorical_columns = dataframe.select_dtypes(include=['object']).columns.tolist()\n",
    "    numeric_columns = dataframe.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "    # Create transformers\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean'))\n",
    "    ])\n",
    "\n",
    "    # Combine transformers using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('cat', categorical_transformer, categorical_columns),\n",
    "        ('num', numeric_transformer, numeric_columns)\n",
    "    ])\n",
    "\n",
    "    # Create the complete pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pipeline(pipeline, df):\n",
    "    transformed_data = pipeline.named_steps['preprocessor'].fit_transform(df)\n",
    "    \n",
    "    # Get column names from OneHotEncoder\n",
    "    categorical_columns = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(input_features=df.select_dtypes(include=['object']).columns.tolist())\n",
    "    column_names = list(categorical_columns) + df.select_dtypes(exclude=['object']).columns.tolist()\n",
    "    \n",
    "    # Convert transformed data to DataFrame with column names\n",
    "    transformed_df = pd.DataFrame(transformed_data, columns=column_names)\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_data(df, categorial_columns):\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(df[categorial_columns])\n",
    "\n",
    "    cat_encoded = encoder.fit_transform(df[categorial_columns]).toarray()\n",
    "    new_columns = encoder.get_feature_names_out(categorial_columns)\n",
    "    cat_encoded_df = pd.DataFrame(cat_encoded, columns=new_columns)\n",
    "    df_encoded = df.drop(columns=categorial_columns).join(cat_encoded_df)\n",
    "\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creation of Model's Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part aims to create a class Model that allows to fit, predict, optimize, score and interprete GB, RF and SVM models for Survival Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a class Scorer \n",
    "class Scorer:\n",
    "    @staticmethod\n",
    "    def concordance_censored(estimator, x_test, y_test, y_train = None):\n",
    "        return concordance_censored(estimator,x_test, y_test)\n",
    "\n",
    "    @staticmethod\n",
    "    def concordance_index_ipcw(estimator,x_test , y_test, y_train):\n",
    "        return concordance_index_ipcw(y_train, y_test, estimator.predict(x_test))[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def ibs(estimator, x_test , y_test, y_train):\n",
    "        if isinstance(estimator, FastKernelSurvivalSVM):\n",
    "            return 'no ibs score for SSVM'\n",
    "        else :\n",
    "            times = list(set(np.percentile(y_test['time'], np.linspace(10, 90, 15))))\n",
    "            survs = estimator.predict_survival_function(x_test)\n",
    "            preds = np.asarray([[fn(t) for t in times] for fn in survs])\n",
    "            return -integrated_brier_score(y_test, y_test, preds, times)     # -ibs because we want to maximise the score to have the best model with optuna\n",
    "\n",
    "    @staticmethod\n",
    "    def cumulative_dynamic_auc(estimator, x_test, y_test, y_train):\n",
    "        times = np.delete(sorted(y_test['time']),-1)\n",
    "        return cumulative_dynamic_auc(y_train, y_test, estimator.predict(x_test), times)[1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(estimator, x_test, y_test, y_train = None):\n",
    "        return accuracy_score(y_test, estimator.predict(x_test))\n",
    "    \n",
    "    @staticmethod\n",
    "    def roc_auc(estimator, x_test, y_test, y_train = None):\n",
    "        return roc_auc_score(y_test, estimator.predict(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    \"\"\"\n",
    "---------------   Initialisation and basic function   ---------------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, classifier):\n",
    "        self.model = classifier\n",
    "        self.random_state = 42\n",
    "        self.params = {'random_state': self.random_state}\n",
    "        self.best_params = {}\n",
    "        self.event = 'event'\n",
    "        self.time = 'time'\n",
    "        if isinstance(self.model, GradientBoostingSurvivalAnalysis) or isinstance(self.model, RandomSurvivalForest) or isinstance(self.model, FastKernelSurvivalSVM):\n",
    "            self.scorer = 'concordance_censored'\n",
    "        elif isinstance(self.model, GradientBoostingClassifier) or isinstance(self.model, RandomForestClassifier) or isinstance(self.model, SVC):\n",
    "            self.scorer = 'roc_auc'\n",
    "\n",
    "    def get_params(self):\n",
    "        self.params['random_state'] = self.random_state\n",
    "        return self.params\n",
    "\n",
    "    def fit(self, x, Y):\n",
    "        if isinstance(self.model, GradientBoostingSurvivalAnalysis) or isinstance(self.model, RandomSurvivalForest) or isinstance(self.model, FastKernelSurvivalSVM):\n",
    "            event, time = Y.dtype.names\n",
    "            self.event = event\n",
    "            self.time = time\n",
    "        self.params['random_state'] = self.random_state\n",
    "        self.model.set_params(**self.params)\n",
    "        self.model.fit(x, Y)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)    \n",
    "    \n",
    "    def score(self, x_test, Y_test, Y_train = None, metrics = None):\n",
    "        if metrics == None :\n",
    "            metrics = [self.scorer]\n",
    "        estimator = self.model\n",
    "        scores = {}\n",
    "        for metric in metrics:\n",
    "            if hasattr(Scorer, metric):\n",
    "                scores[metric] = getattr(Scorer, metric)(estimator, x_test, Y_test, Y_train)\n",
    "            else:\n",
    "                raise ValueError(f\"Metric '{metric}' is not a valid scoring metric./n Metric possible are : concordance_index_censured, concordance_index_ipcw, cumulative_dynamic_auc and ibs\")\n",
    "        return scores\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "---------------   Cross-validation score of the model   ---------------\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def k_fold_cross_validation(self, x, Y, k=5):\n",
    "        \"\"\"\n",
    "        Performs k-fold cross-validation for a given model and dataset.\n",
    "\n",
    "        Parameters:\n",
    "            model: The machine learning model to evaluate.\n",
    "            X (numpy.ndarray): The feature matrix.\n",
    "            y (numpy.ndarray): The target vector.\n",
    "            k (int): Number of folds for cross-validation.\n",
    "\n",
    "        Returns:\n",
    "            float: The average accuracy across all folds.\n",
    "        \"\"\"\n",
    "        n = x.shape[0]\n",
    "        fold_size = n // k\n",
    "        scores = []\n",
    "\n",
    "        for i in range(k):\n",
    "            # Splitting data into training and validation sets\n",
    "            validation_X = pd.DataFrame(x[i * fold_size: (i + 1) * fold_size], columns=x.columns)\n",
    "            validation_y = Y[i * fold_size: (i + 1) * fold_size]\n",
    "            train_X = pd.DataFrame(np.concatenate([x[:i * fold_size], x[(i + 1) * fold_size:]]), columns=x.columns)\n",
    "            train_y = np.concatenate([Y[:i * fold_size], Y[(i + 1) * fold_size:]])\n",
    "\n",
    "            # Fitting the model\n",
    "            self.fit(train_X, train_y)\n",
    "\n",
    "            # Calculating accuracy\n",
    "            score = self.score(validation_X, validation_y, train_y, metrics = [self.scorer])[self.scorer]\n",
    "            scores.append(score)\n",
    "\n",
    "        # Returning the average accuracy\n",
    "        return sum(scores) / k\n",
    "\n",
    "    \"\"\"\n",
    "---------------   Optimization of the model   ---------------\n",
    "    \"\"\"\n",
    "\n",
    "    def optimize_with_optuna(self, x, Y, n_trials = 50, plot = False):\n",
    "        list_score = []\n",
    "        def objective(trial):\n",
    "            # Define search space for hyperparameters\n",
    "            params = {}\n",
    "            if isinstance(self.model, GradientBoostingSurvivalAnalysis) or isinstance(self.model, GradientBoostingClassifier):\n",
    "                params['learning_rate'] = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n",
    "                params['n_estimators'] = trial.suggest_int('n_estimators', 50, 400)\n",
    "                params['max_depth'] = trial.suggest_int('max_depth', 3, 10)\n",
    "                params['subsample'] = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "                params['min_samples_split'] = trial.suggest_int('min_samples_split', 2, 20)\n",
    "                params['min_samples_leaf'] = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "            elif isinstance(self.model, RandomSurvivalForest) or isinstance(self.model, RandomForestClassifier):\n",
    "                params['n_estimators'] = trial.suggest_int('n_estimators', 50, 400)\n",
    "                params['max_depth'] = trial.suggest_int('max_depth', 3, 10)\n",
    "                params['max_samples'] = trial.suggest_float('max_samples', 0.1, 1.0)\n",
    "                params['min_samples_split'] = trial.suggest_int('min_samples_split', 2, 20)\n",
    "                params['min_samples_leaf'] = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "            elif isinstance(self.model, SVC):\n",
    "                params['max_iter'] = 1000\n",
    "                params['C'] = trial.suggest_int('C', 1e-5, 1e5)\n",
    "                params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "                params['gamma'] = trial.suggest_float('gamma', 1e-5, 1e3,log=True)\n",
    "                params['kernel'] = trial.suggest_categorical('kernel', ['linear','poly'])\n",
    "            elif isinstance(self.model, FastKernelSurvivalSVM):\n",
    "                params['max_iter'] = 1000\n",
    "                params['alpha'] = trial.suggest_float('alpha', 0.01, 100)\n",
    "                params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "                params['gamma'] = trial.suggest_float('gamma', 1e-5, 1e3,log=True)\n",
    "                params['kernel'] = trial.suggest_categorical('kernel', ['linear','poly'])\n",
    "\n",
    "            # Initialize model with hyperparameters\n",
    "            if isinstance(self.model, SVC):\n",
    "                self.model = self.model.set_params(**params, random_state = self.random_state, probability = True)\n",
    "            else : \n",
    "                self.model = self.model.set_params(**params, random_state = self.random_state)\n",
    "\n",
    "            score = self.k_fold_cross_validation(x, Y, k=5)\n",
    "            if len(list_score)==0:\n",
    "                list_score.append(score)\n",
    "            else : \n",
    "                list_score.append(max(score,max(list_score)))\n",
    "\n",
    "            return score\n",
    "        \n",
    "        # Create Optuna study object\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "\n",
    "        # Run optimization\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "        # Access best hyperparameters\n",
    "        best_params = study.best_params\n",
    "        self.model.set_params(**best_params)\n",
    "        self.params = best_params\n",
    "        \n",
    "        # Displays of best_params in the os\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(f'Best hyperparameters with optuna : {best_params}')\n",
    "\n",
    "        if plot:\n",
    "            plt.plot(list_score)\n",
    "            plt.xlabel('Numbers of iterations')\n",
    "            plt.ylabel('Model score')\n",
    "            plt.title('Evolution of the model score during optuna optimisation')\n",
    "            plt.show()\n",
    "        \n",
    "        return best_params, list_score\n",
    "    \n",
    "    def optimize(self, x, Y, num_samples = 3, n_trials = 50, plot = False):\n",
    "        # Define the gridsearch space\n",
    "        optimal_params, list_score = self.optimize_with_optuna(x,Y, n_trials)\n",
    "        param_grid = generate_param_grid(optimal_params, num_samples = num_samples)\n",
    "        if isinstance(self.model, GradientBoostingSurvivalAnalysis) or isinstance(self.model, RandomSurvivalForest):\n",
    "            param_grid['min_samples_leaf'][param_grid['min_samples_leaf'] != 1]\n",
    "            param_grid['min_samples_split'][param_grid['min_samples_split'] != 1]\n",
    "                \n",
    "        # Initialize the model\n",
    "        best_score = 0\n",
    "        best_params = None\n",
    "        nb_comb = count_combinations(param_grid)\n",
    "        progress_bar = tqdm(total=nb_comb, desc=\"Progress of GridSearchCV\")\n",
    "\n",
    "        for params in get_param_combinations(param_grid) : \n",
    "            # Create model instance with current hyperparameters\n",
    "            self.model = self.model.set_params(**params, random_state = self.random_state) \n",
    "\n",
    "            if self.k_fold_cross_validation(x, Y, k=5) > best_score:\n",
    "                best_score = self.k_fold_cross_validation(x, Y, k=5)\n",
    "                best_params = params\n",
    "                \n",
    "            progress_bar.update(1)\n",
    "\n",
    "        self.model.set_params(**best_params)\n",
    "        self.params = best_params\n",
    "        self.best_params = best_params\n",
    "        self.params['random_state'] = self.random_state\n",
    "        progress_bar.close()\n",
    "\n",
    "        if plot:\n",
    "            plt.plot(list_score)\n",
    "            plt.xlabel('Numbers of iterations')\n",
    "            plt.ylabel('Model score')\n",
    "            plt.title('Evolution of the model score during optuna optimisation')\n",
    "            plt.show()\n",
    "\n",
    "        # Displays of best_params in the os\n",
    "        print(f'Best hyperparameters with optuna - GridSearch : {Fore.BLUE}{best_params}{Style.RESET_ALL} \\nwith a score: {Fore.BLUE}{best_score}{Style.RESET_ALL}; and the scorer: {Fore.BLUE}{self.scorer}{Style.RESET_ALL}')\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "---------------   Interpretability methods   ---------------\n",
    "    \"\"\"\n",
    "\n",
    "    def get_interpretability_methods(self, x_train, x_test, Y_train, Y_test, feature = None, index = 0, plot = False):\n",
    "        interpretability_methods = {\n",
    "            'SHAP': self.get_shap_values(x_train, x_test, feature, plot),\n",
    "            'LIME': self.get_lime_explanation(x_train, x_test, index),\n",
    "            'PI': self.get_pi_values(x_test, Y_test, Y_train)\n",
    "        }\n",
    "        return interpretability_methods\n",
    "\n",
    "    def get_shap_values(self, x_train, x_test, feature = None, plot = False):\n",
    "        if isinstance(self.model, GradientBoostingSurvivalAnalysis) or isinstance(self.model, RandomSurvivalForest) or isinstance(self.model, RandomForestClassifier) or isinstance(self.model, GradientBoostingClassifier) :\n",
    "            # use Tree Explainer SHAP to explain test set predictions\n",
    "            explainer = shap.Explainer(self.predict, x_train)\n",
    "            shap_values = explainer.shap_values(x_test)\n",
    "            \n",
    "            if plot :\n",
    "                # Display SHAP's summary plot\n",
    "                shap.summary_plot(shap_values,x_test)\n",
    "            \n",
    "            # Display SHAP's dependance plot\n",
    "            if feature != None : \n",
    "                shap.dependence_plot(feature, shap_values, x_test)\n",
    "\n",
    "            return shap_values\n",
    "        \n",
    "        elif isinstance(self.model, FastKernelSurvivalSVM) or isinstance(self.model, SVC):\n",
    "            # use Kernel SHAP to explain test set predictions\n",
    "            explainer = shap.KernelExplainer(self.predict, x_train)\n",
    "            shap_values = explainer.shap_values(x_test)\n",
    "\n",
    "            # Display SHAP's summary plot\n",
    "            shap.summary_plot(shap_values,x_test)\n",
    "            \n",
    "            # Display SHAP's dependance plot\n",
    "            if feature != None : \n",
    "                shap.dependence_plot(feature, shap_values, x_test)\n",
    "\n",
    "            return shap_values\n",
    "\n",
    "    def permutation_importance_feature(self, x_test, Y_test, Y_train, n_permutations=100):\n",
    "        baseline_score = self.score(x_test, Y_test, Y_train)[self.scorer]\n",
    "        feature_importance = {}\n",
    "        \n",
    "        for i in range(x_test.shape[1]):\n",
    "            scores = []\n",
    "            for j in range(n_permutations):\n",
    "                x_permuted = x_test.copy()\n",
    "                x_permuted.iloc[:, i] = np.random.permutation(x_permuted.iloc[:, i])\n",
    "                score = self.score(x_permuted, Y_test, Y_train)[self.scorer]\n",
    "                scores.append(score)\n",
    "            feature_importance[x_test.columns[i]] = abs(baseline_score - np.mean(scores))\n",
    "        \n",
    "        return feature_importance\n",
    "\n",
    "\n",
    "    def get_pi_values(self, x_test, Y_test, Y_train, plot = False, n_permutations = 100):\n",
    "        feature_importances = self.permutation_importance_feature(x_test, Y_test, Y_train, n_permutations)\n",
    "        sorted_importances = {k: v for k, v in sorted(feature_importances.items(), key=lambda item: item[1])}\n",
    "        total_importance = sum(sorted_importances.values())\n",
    "        importance_percent = {k: v / total_importance * 100 for k, v in sorted_importances.items()}\n",
    "        n_features = len(feature_importances)\n",
    "        if plot:\n",
    "            # Display\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(list(importance_percent.keys())[n_features-10:], list(importance_percent.values())[n_features-10:], color='blue')\n",
    "            plt.xlabel('Importance (%)')\n",
    "            plt.ylabel('Variable')\n",
    "            plt.title('Variables importance')\n",
    "            plt.show()\n",
    "\n",
    "        return importance_percent\n",
    "            \n",
    "        \n",
    "\n",
    "    def get_lime_explanation(self, x_train, x_test, Y_test, sample_idx = [0]):\n",
    "        if isinstance(self.model, RandomForestClassifier) or isinstance(self.model, GradientBoostingClassifier) or isinstance(self.model, SVC):        \n",
    "            # Initialize LIME explainer\n",
    "            explainer = lime.lime_tabular.LimeTabularExplainer(training_data = x_train.values, feature_names = x_train.columns, class_names=['event_1','event_0'], mode = 'classification')\n",
    "\n",
    "            for index in sample_idx:\n",
    "                # Select instance to explain\n",
    "                instance_idx = index\n",
    "\n",
    "                # Explain prediction\n",
    "                explanation = explainer.explain_instance(x_test.iloc[instance_idx], self.model.predict_proba)\n",
    "\n",
    "                # Show explanation\n",
    "                print(f'Lime explanation for index {index}')\n",
    "                explanation.show_in_notebook()\n",
    "        \n",
    "        elif isinstance(self.model, FastKernelSurvivalSVM):\n",
    "            # Let's use LIME method to interpret this model, since the output of a SSVM is a risk score, we can use classical lime method to interpret it\n",
    "\n",
    "            # Initialize the explainer\n",
    "            explainer = lime.lime_tabular.LimeTabularExplainer(training_data = x_train.values, mode='regression', feature_names=x_train.columns)\n",
    "\n",
    "            for index in sample_idx:\n",
    "\n",
    "                # Explain the sample\n",
    "                explanation = explainer.explain_instance(x_test.iloc[index], self.predict, num_features=5)\n",
    "\n",
    "                # Display the explanation\n",
    "                print('Sample  : ', index)\n",
    "                print('Time   : ', Y_test[index][1])\n",
    "                print('Event   :'  , Y_test[index][0])\n",
    "                explanation.show_in_notebook()\n",
    "                \n",
    "    \"\"\"\n",
    "---------------   Hyperparameters importance   ---------------\n",
    "    \"\"\"\n",
    "\n",
    "    def hyperparameters_importances(self, hyperparameters_range, x, Y, n_trials = 50, n_samples = 15, plot = False):\n",
    "        self.params = {}\n",
    "        base_score = self.k_fold_cross_validation(x,Y)\n",
    "        importance = {}\n",
    "        n_sim = n_trials * n_samples * (len(hyperparameters_range) - 1)\n",
    "        progress_bar = tqdm(total = n_sim, desc = \"Progress of Hyperparameters importance\")\n",
    "        for param_name, param_values in hyperparameters_range.items():\n",
    "            print(f'Hyperparameters : {param_name}')\n",
    "            importance[param_name] = 0\n",
    "            score_diff = 0\n",
    "            if isinstance(param_values[0], str):\n",
    "                score_diff = 0\n",
    "                for value in param_values:\n",
    "                    for _ in range(n_trials):\n",
    "                        # Create random params for others hyperparameters\n",
    "                        hyp_dict = {key: value for key, value in hyperparameters_range.items() if key != param_name}\n",
    "                        self.params = random_number_dict(hyp_dict)\n",
    "                        \n",
    "                        # Create a model with the specified hyperparameter\n",
    "                        self.params[param_name] = value\n",
    "\n",
    "                        # Evaluate the modified model:\n",
    "                        modified_score = self.k_fold_cross_validation(x, Y)\n",
    "                        diff = base_score - modified_score\n",
    "                        score_diff += abs(diff)\n",
    "                        progress_bar.update(1)\n",
    "\n",
    "                    # Average score differences over trials\n",
    "                    score_difference_avg = score_diff / n_trials\n",
    "\n",
    "                    # Add the average score difference to the hyperparameter's importance\n",
    "                    importance[param_name] += abs(score_difference_avg)\n",
    "\n",
    "            elif isinstance(param_values[0], int) : \n",
    "                step = int((param_values[1] + 1 - param_values[0])/n_samples) + 1\n",
    "                for value in range(param_values[0], param_values[1] + 1, step):\n",
    "                    for _ in range(n_trials):   \n",
    "                        # Create random params for others hyperparameters\n",
    "                        hyp_dict = {key: value for key, value in hyperparameters_range.items() if key != param_name}\n",
    "                        self.params = random_number_dict(hyp_dict)\n",
    "                        \n",
    "                        # Create a model with the specified hyperparameter\n",
    "                        self.params[param_name] = value\n",
    "\n",
    "                        # Evaluate the modified model:\n",
    "                        modified_score = self.k_fold_cross_validation(x, Y)\n",
    "                        diff = base_score - modified_score\n",
    "                        score_diff += abs(diff)\n",
    "                        progress_bar.update(1)\n",
    "\n",
    "                    # Average score differences over trials\n",
    "                    score_difference_avg = score_diff / n_trials\n",
    "\n",
    "                    # Add the average score difference to the hyperparameter's importance\n",
    "                    importance[param_name] += abs(score_difference_avg)\n",
    "\n",
    "            elif isinstance(param_values[0], float):\n",
    "                for value in generate_random_numbers(param_values[0], param_values[1], n_samples):\n",
    "                    for _ in range(n_trials):\n",
    "                        # Create random params for others hyperparameters\n",
    "                        hyp_dict = {key: value for key, value in hyperparameters_range.items() if key != param_name}\n",
    "                        self.params = random_number_dict(hyp_dict)\n",
    "                        \n",
    "                        # Create a model with the specified hyperparameter\n",
    "                        self.params[param_name] = value\n",
    "\n",
    "                        # Evaluate the modified model:\n",
    "                        modified_score = self.k_fold_cross_validation(x, Y)\n",
    "                        diff = base_score - modified_score\n",
    "                        score_diff += abs(diff)\n",
    "                        progress_bar.update(1)\n",
    "\n",
    "                    # Average score differences over trials\n",
    "                    score_difference_avg = score_diff / n_trials\n",
    "\n",
    "                    # Add the average score difference to the hyperparameter's importance\n",
    "                    importance[param_name] += abs(score_difference_avg)\n",
    "        \n",
    "        progress_bar.close()\n",
    "        sorted_importance = {k: v for k, v in sorted(importance.items(), key=lambda item: item[1])}\n",
    "\n",
    "        if plot : \n",
    "            # Display\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(list(sorted_importance.keys()), list(sorted_importance.values()), color='blue')\n",
    "            plt.xlabel('Importance')\n",
    "            plt.ylabel('Hyperparameters')\n",
    "            plt.title('Hyperparameters importance')\n",
    "            plt.show()\n",
    "\n",
    "        self.params = self.best_params\n",
    "        self.fit(x,Y)\n",
    "        return sorted_importance\n",
    "    \n",
    "    \"\"\"\n",
    "---------------   Learning curve   ---------------\n",
    "    \"\"\"\n",
    "    \n",
    "    def learning_curve(self, X_train, X_test, y_train, y_test, n_sample = 25):\n",
    "        train_sizes, train_scores, test_scores = [], [], []\n",
    "\n",
    "        # Iterate over different training set sizes\n",
    "        for train_size in np.linspace(0.1, 1.0, n_sample):\n",
    "            train_size = int(train_size * len(X_train))\n",
    "            train_sizes.append(train_size)\n",
    "\n",
    "            # Train the model on a subset of the training set\n",
    "            self.fit(X_train[:train_size], y_train[:train_size])\n",
    "\n",
    "            # Calculate scores on the training and validation sets\n",
    "            train_score = self.score(X_train[:train_size], y_train[:train_size], y_train[:train_size])[self.scorer]\n",
    "            test_score = self.score(X_test, y_test, y_train)[self.scorer]\n",
    "\n",
    "            train_scores.append(train_score)\n",
    "            test_scores.append(test_score)\n",
    "\n",
    "        # Plot the learning curve\n",
    "        plt.figure()\n",
    "        plt.title(\"Learning Curve\")\n",
    "        plt.xlabel(\"Training Examples\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.plot(train_sizes, train_scores, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.plot(train_sizes, test_scores, 'o-', color=\"g\", label=\"Validation score\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "---------------   Accuracy of use of machine learning in survival analysis   ---------------\n",
    "    \"\"\"\n",
    "\n",
    "    def compare_to_cox(self, df, x_test, Y_test, Y_train):\n",
    "\n",
    "        if isinstance(self.model, GradientBoostingSurvivalAnalysis) or isinstance(self.model, RandomSurvivalForest) or isinstance(self.model, FastKernelSurvivalSVM):\n",
    "            # Create Cox model\n",
    "            cox_model = CoxPHFitter()\n",
    "\n",
    "            # Fit the model, for each categorical feature we drop the last category to avoid multicollinearity\n",
    "            cox_model.fit(df, duration_col=self.time, event_col=self.event)\n",
    "\n",
    "            if hasattr(Scorer, self.scorer):\n",
    "                cox_score = concordance_index(df[self.time], -cox_model.predict_partial_hazard(df), df[self.event])\n",
    "            else:\n",
    "                raise ValueError(f\"Metric '{self.scorer}' is not a valid scoring metric./n Metric possible are : concordance_index_censured, concordance_index_ipcw, cumulative_dynamic_auc and ibs\")\n",
    "            \n",
    "            # Compare the two models : \n",
    "            ml_score = self.score(x_test, Y_test, Y_train, metrics = ['concordance_censored'])['concordance_censored']\n",
    "\n",
    "            diff = abs(ml_score - cox_score)\n",
    "            diff_relative = diff/cox_score\n",
    "            if ml_score > cox_score :\n",
    "                print(f'The ML model improves the score of',Fore.RED + f'{round(diff_relative * 100,2)}%' + Style.RESET_ALL, 'compared with the Cox model, with a score of {round(ml_score,3)} for the ML model and {round(cox_score,3)} for cox model')\n",
    "            else : \n",
    "                print(f'the Cox model should be preferred to the ML model, as the latter has a lower score of around',Fore.RED + f'{round(diff_relative * 100,2)}%' + Style.RESET_ALL,'compared with the Cox model, with a score of {round(ml_score,3)} for the ML model and {round(cox_score,3)} for cox model')\n",
    "\n",
    "        else :\n",
    "            raise ValueError('No comparaison with Cox for Classification models')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
