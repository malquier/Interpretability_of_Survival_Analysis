{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **This notebook aims to study interpretability methods for Survival Gradient Boosting ML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Standard ML import\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import optuna\n",
    "from itertools import product\n",
    "\n",
    "# Survival Analysis tools\n",
    "from sksurv.metrics import concordance_index_censored,cumulative_dynamic_auc,concordance_index_ipcw\n",
    "from sksurv.metrics import as_concordance_index_ipcw_scorer, as_cumulative_dynamic_auc_scorer, as_integrated_brier_score_scorer\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.datasets import get_x_y\n",
    "from lifelines import CoxPHFitter\n",
    "from survlimepy import SurvLimeExplainer\n",
    "\n",
    "# Interpretability tools\n",
    "\n",
    "import shap\n",
    "import lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Read and processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>pren_prod</th>\n",
       "      <th>pren_comp</th>\n",
       "      <th>sex_0</th>\n",
       "      <th>sex_1</th>\n",
       "      <th>smoker_0</th>\n",
       "      <th>smoker_1</th>\n",
       "      <th>point_sales_0</th>\n",
       "      <th>point_sales_1</th>\n",
       "      <th>point_sales_2</th>\n",
       "      <th>...</th>\n",
       "      <th>pay_freq_2</th>\n",
       "      <th>pay_freq_3</th>\n",
       "      <th>pay_method_0</th>\n",
       "      <th>pay_method_1</th>\n",
       "      <th>pay_method_2</th>\n",
       "      <th>profession_0</th>\n",
       "      <th>profession_1</th>\n",
       "      <th>profession_2</th>\n",
       "      <th>evento</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>780.00</td>\n",
       "      <td>1.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>609.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>52.78</td>\n",
       "      <td>16.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>757.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>63.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>672.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>19.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>351.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>632.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  pren_prod  pren_comp  sex_0  sex_1  smoker_0  smoker_1  point_sales_0  \\\n",
       "0   40     780.00       1.88    1.0    0.0       0.0       1.0            0.0   \n",
       "1   43      52.78      16.88    0.0    1.0       0.0       1.0            1.0   \n",
       "2   52      63.50       0.00    1.0    0.0       0.0       1.0            1.0   \n",
       "3   25      19.10       0.00    0.0    1.0       0.0       1.0            0.0   \n",
       "4   51     351.00       0.00    1.0    0.0       0.0       1.0            1.0   \n",
       "\n",
       "   point_sales_1  point_sales_2  ...  pay_freq_2  pay_freq_3  pay_method_0  \\\n",
       "0            1.0            0.0  ...         0.0         1.0           0.0   \n",
       "1            0.0            0.0  ...         0.0         1.0           0.0   \n",
       "2            0.0            0.0  ...         0.0         1.0           0.0   \n",
       "3            1.0            0.0  ...         0.0         1.0           0.0   \n",
       "4            0.0            0.0  ...         0.0         1.0           0.0   \n",
       "\n",
       "   pay_method_1  pay_method_2  profession_0  profession_1  profession_2  \\\n",
       "0           1.0           0.0           0.0           0.0           1.0   \n",
       "1           1.0           0.0           0.0           1.0           0.0   \n",
       "2           1.0           0.0           0.0           1.0           0.0   \n",
       "3           1.0           0.0           0.0           1.0           0.0   \n",
       "4           1.0           0.0           0.0           1.0           0.0   \n",
       "\n",
       "   evento   time  \n",
       "0   False  609.0  \n",
       "1    True  757.0  \n",
       "2    True  672.0  \n",
       "3    True  407.0  \n",
       "4    True  632.0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset Insurance\n",
    "data_ins = pd.read_csv(\"X_train.csv\")\n",
    "ins_credit = pd.read_csv(\"y_train.csv\")\n",
    "\n",
    "data_ins.rename(columns = {'0':'age','1':'sex', '2':'smoker', '3':'pren_prod', '4':'pren_comp', '5':'point_sales', '6': 'product_type', '7': 'dist_channel', '8': 'pay_freq', '9': 'pay_method', '10':'profession'}, inplace = True)\n",
    "\n",
    "\n",
    "# Categorical columns\n",
    "categorial_columns = ['sex', 'smoker', 'point_sales', 'product_type', 'dist_channel', 'pay_freq', 'pay_method', 'profession']\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(data_ins[categorial_columns])\n",
    "\n",
    "ins_encoded = encoder.fit_transform(data_ins[categorial_columns]).toarray()\n",
    "new_columns = encoder.get_feature_names_out(categorial_columns)\n",
    "ins_encoded_df = pd.DataFrame(ins_encoded, columns=new_columns)\n",
    "ins_features = data_ins.drop(columns=categorial_columns).join(ins_encoded_df)\n",
    "\n",
    "df = pd.concat([ins_features, ins_credit], axis=1).drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove negative time rows\n",
    "for ind in df.index:\n",
    "    if df['time'][ind] < 0:\n",
    "        df = df.drop(ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split\n",
    "X,y = get_x_y(df,attr_labels=['evento','time'],pos_label=1,survival=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cox Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     coef      exp(coef)     se(coef)  coef lower 95%  \\\n",
      "covariate                                                               \n",
      "age             -0.016833       0.983308     0.001710       -0.020183   \n",
      "pren_prod       -0.000066       0.999934     0.000040       -0.000145   \n",
      "pren_comp        0.000019       1.000019     0.000035       -0.000049   \n",
      "sex_0            0.051441       1.052787     0.030198       -0.007745   \n",
      "smoker_0         0.111786       1.118273     0.059501       -0.004834   \n",
      "point_sales_0   -0.084546       0.918929     0.149576       -0.377710   \n",
      "point_sales_1    0.219518       1.245477     0.148433       -0.071404   \n",
      "point_sales_2    0.121547       1.129243     0.155351       -0.182935   \n",
      "point_sales_3    0.136411       1.146153     0.157423       -0.172131   \n",
      "product_type_0  13.162454  520452.383998   253.957309     -484.584725   \n",
      "product_type_1  13.009031  446427.031988   253.957314     -484.738159   \n",
      "product_type_2  12.813034  366969.532533   253.957300     -484.934128   \n",
      "product_type_3  13.030739  456223.791244   253.957304     -484.716431   \n",
      "product_type_4  12.233601  205582.204887   253.959263     -485.517408   \n",
      "product_type_5   0.296712       1.345428  1055.459411    -2068.365719   \n",
      "product_type_6  -0.038657       0.962081  1153.962063    -2261.762740   \n",
      "product_type_7  12.956740  423682.661204   253.957298     -484.790418   \n",
      "product_type_8  12.733008  338746.703227   253.957298     -485.014150   \n",
      "product_type_9  13.130003  503834.579668   253.957540     -484.617629   \n",
      "dist_channel_0  -0.295448       0.744198     0.467098       -1.210944   \n",
      "dist_channel_1  -0.575775       0.562269     0.456577       -1.470650   \n",
      "dist_channel_2  -0.206999       0.813021     0.455960       -1.100664   \n",
      "dist_channel_3  -0.015076       0.985037     0.465747       -0.927923   \n",
      "pay_freq_0       0.023122       1.023391     0.059705       -0.093897   \n",
      "pay_freq_1      -0.111210       0.894750     0.050904       -0.210981   \n",
      "pay_freq_2       0.772844       2.165916     0.713009       -0.624629   \n",
      "pay_method_0    -1.612974       0.199294     0.709887       -3.004327   \n",
      "pay_method_1    -2.077702       0.125218     0.708656       -3.466641   \n",
      "profession_0     0.247877       1.281302     0.074257        0.102336   \n",
      "profession_1    -0.210911       0.809846     0.037453       -0.284317   \n",
      "\n",
      "                coef upper 95%  exp(coef) lower 95%  exp(coef) upper 95%  \\\n",
      "covariate                                                                  \n",
      "age                  -0.013482         9.800191e-01         9.866086e-01   \n",
      "pren_prod             0.000013         9.998549e-01         1.000013e+00   \n",
      "pren_comp             0.000087         9.999507e-01         1.000087e+00   \n",
      "sex_0                 0.110627         9.922844e-01         1.116978e+00   \n",
      "smoker_0              0.228406         9.951773e-01         1.256595e+00   \n",
      "point_sales_0         0.208617         6.854294e-01         1.231974e+00   \n",
      "point_sales_1         0.510441         9.310854e-01         1.666025e+00   \n",
      "point_sales_2         0.426030         8.328225e-01         1.531166e+00   \n",
      "point_sales_3         0.444954         8.418685e-01         1.560418e+00   \n",
      "product_type_0      510.909633        3.527993e-211        7.677755e+221   \n",
      "product_type_1      510.756221        3.026165e-211        6.585798e+221   \n",
      "product_type_2      510.560196        2.487620e-211        5.413473e+221   \n",
      "product_type_3      510.777909        3.092634e-211        6.730190e+221   \n",
      "product_type_4      509.984610        1.388254e-211        3.044403e+221   \n",
      "product_type_5     2068.959144         0.000000e+00                  inf   \n",
      "product_type_6     2261.685426         0.000000e+00                  inf   \n",
      "product_type_7      510.703898        2.872081e-211        6.250068e+221   \n",
      "product_type_8      510.480166        2.296313e-211        4.997112e+221   \n",
      "product_type_9      510.877635        3.413800e-211        7.435974e+221   \n",
      "dist_channel_0        0.620048         2.979158e-01         1.859017e+00   \n",
      "dist_channel_1        0.319100         2.297762e-01         1.375888e+00   \n",
      "dist_channel_2        0.686666         3.326502e-01         1.987080e+00   \n",
      "dist_channel_3        0.897770         3.953742e-01         2.454125e+00   \n",
      "pay_freq_0            0.140141         9.103767e-01         1.150435e+00   \n",
      "pay_freq_1           -0.011440         8.097898e-01         9.886249e-01   \n",
      "pay_freq_2            2.170316         5.354601e-01         8.761052e+00   \n",
      "pay_method_0         -0.221622         4.957210e-02         8.012183e-01   \n",
      "pay_method_1         -0.688762         3.122171e-02         5.021974e-01   \n",
      "profession_0          0.393418         1.107756e+00         1.482038e+00   \n",
      "profession_1         -0.137505         7.525281e-01         8.715299e-01   \n",
      "\n",
      "                cmp to         z             p   -log2(p)  \n",
      "covariate                                                  \n",
      "age                0.0 -9.846114  7.124572e-23  73.571543  \n",
      "pren_prod          0.0 -1.647086  9.954043e-02   3.328573  \n",
      "pren_comp          0.0  0.537494  5.909267e-01   0.758949  \n",
      "sex_0              0.0  1.703470  8.848006e-02   3.498504  \n",
      "smoker_0           0.0  1.878716  6.028334e-02   4.052097  \n",
      "point_sales_0      0.0 -0.565239  5.719112e-01   0.806137  \n",
      "point_sales_1      0.0  1.478909  1.391647e-01   2.845135  \n",
      "point_sales_2      0.0  0.782406  4.339761e-01   1.204312  \n",
      "point_sales_3      0.0  0.866528  3.862005e-01   1.372578  \n",
      "product_type_0     0.0  0.051829  9.586646e-01   0.060902  \n",
      "product_type_1     0.0  0.051225  9.591460e-01   0.060178  \n",
      "product_type_2     0.0  0.050453  9.597610e-01   0.059253  \n",
      "product_type_3     0.0  0.051311  9.590779e-01   0.060280  \n",
      "product_type_4     0.0  0.048172  9.615796e-01   0.056522  \n",
      "product_type_5     0.0  0.000281  9.997757e-01   0.000324  \n",
      "product_type_6     0.0 -0.000033  9.999733e-01   0.000039  \n",
      "product_type_7     0.0  0.051019  9.593101e-01   0.059931  \n",
      "product_type_8     0.0  0.050138  9.600121e-01   0.058875  \n",
      "product_type_9     0.0  0.051702  9.587665e-01   0.060749  \n",
      "dist_channel_0     0.0 -0.632518  5.270486e-01   0.923992  \n",
      "dist_channel_1     0.0 -1.261069  2.072841e-01   2.270318  \n",
      "dist_channel_2     0.0 -0.453984  6.498401e-01   0.621843  \n",
      "dist_channel_3     0.0 -0.032370  9.741770e-01   0.037744  \n",
      "pay_freq_0         0.0  0.387272  6.985550e-01   0.517554  \n",
      "pay_freq_1         0.0 -2.184705  2.891047e-02   5.112264  \n",
      "pay_freq_2         0.0  1.083918  2.784011e-01   1.844763  \n",
      "pay_method_0       0.0 -2.272157  2.307701e-02   5.437400  \n",
      "pay_method_1       0.0 -2.931892  3.369043e-03   8.213446  \n",
      "profession_0       0.0  3.338101  8.435316e-04  10.211270  \n",
      "profession_1       0.0 -5.631399  1.787542e-08  25.737448  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Column pay_freq_2 have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['evento'].astype(bool)\n",
      ">>> print(df.loc[events, 'pay_freq_2'].var())\n",
      ">>> print(df.loc[~events, 'pay_freq_2'].var())\n",
      "\n",
      "A very low variance means that the column pay_freq_2 completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "Newton-Raphson convergence completed successfully but norm(delta) is still high, 0.833. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Cox model\n",
    "cox_model = CoxPHFitter()\n",
    "\n",
    "# Fit the model, for each categorical feature we drop the last category to avoid multicollinearity\n",
    "cox_model.fit(df.drop(columns=['sex_1','smoker_1','point_sales_4','product_type_10','dist_channel_4','pay_freq_3','pay_method_2','profession_2']), duration_col='time', event_col='evento')\n",
    "\n",
    "# Display the summary of the model\n",
    "print(cox_model.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c-index of the cox model\n",
    "\n",
    "c_index = concordance_index_censored(y_train['evento'], y_train['time'], cox_model.predict_expectation(X_train))\n",
    "print(\"C-index: \", c_index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gradient Boosting Survival Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingSurvivalAnalysis(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingSurvivalAnalysis</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingSurvivalAnalysis(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingSurvivalAnalysis(random_state=42)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit the survival svm\n",
    "\n",
    "SGB = GradientBoostingSurvivalAnalysis(random_state=42)\n",
    "SGB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_index : 0.6088186035041973\n",
      "c_index_ipcw : 0.6072029301052714\n",
      "auc_dynamic : 0.6322269195865358\n"
     ]
    }
   ],
   "source": [
    "# define the concordance index\n",
    "\n",
    "def concordance_censored(estimator,X,y):\n",
    "    concordance = concordance_index_censored([elt[0] for elt in y],[elt[1] for elt in y],estimator.predict(X))\n",
    "    return concordance[0]\n",
    "\n",
    "print(f\"c_index : {concordance_censored(SGB,X_test,y_test)}\")\n",
    "\n",
    "# define the concordance index ipcw\n",
    "\n",
    "concordance_ipcw = concordance_index_ipcw(y_train,y_test,SGB.predict(X_test))\n",
    "print(f\"c_index_ipcw : {concordance_ipcw[0]}\")\n",
    "\n",
    "# compute the auc dynamic score\n",
    "\n",
    "auc_dynamic = cumulative_dynamic_auc(y_train,y_test,SGB.predict(X_test),times = np.arange(1, 2059, 30))\n",
    "print(f\"auc_dynamic : {auc_dynamic[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SGB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the time-dependent AUC\u001b[39;00m\n\u001b[1;32m      2\u001b[0m times \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m], np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m12\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m auc, mean_auc \u001b[38;5;241m=\u001b[39m cumulative_dynamic_auc(y_train, y_test, \u001b[43mSGB\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test), times \u001b[38;5;241m=\u001b[39m times)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(times, auc, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdays from enrollment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SGB' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the time-dependent AUC\n",
    "times = np.percentile(df['time'], np.linspace(15, 90, 12))\n",
    "auc, mean_auc = cumulative_dynamic_auc(y_train, y_test, SGB.predict(X_test), times = times)\n",
    "plt.plot(times, auc, marker=\"o\", color='blue')\n",
    "plt.xlabel(\"days from enrollment\")\n",
    "plt.ylabel(\"time-dependent AUC\")\n",
    "plt.axhline(mean_auc, linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameters optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-18 14:03:59,026] A new study created in memory with name: no-name-6666526c-cd2d-4151-9b30-dcc3214b2a86\n",
      "X has feature names, but GradientBoostingSurvivalAnalysis was fitted without feature names\n",
      "X has feature names, but GradientBoostingSurvivalAnalysis was fitted without feature names\n",
      "X has feature names, but GradientBoostingSurvivalAnalysis was fitted without feature names\n",
      "X has feature names, but GradientBoostingSurvivalAnalysis was fitted without feature names\n",
      "X has feature names, but GradientBoostingSurvivalAnalysis was fitted without feature names\n",
      "X has feature names, but GradientBoostingSurvivalAnalysis was fitted without feature names\n",
      "X has feature names, but GradientBoostingSurvivalAnalysis was fitted without feature names\n",
      "X has feature names, but GradientBoostingSurvivalAnalysis was fitted without feature names\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters optimization with optuna\n",
    "\n",
    "# We use the concordance index as evaluation metric\n",
    "def scorer(estimator, X, y):\n",
    "    concordance =  concordance_index_censored([elt[0] for elt in y],[elt[1] for elt in y],estimator.predict(X))[0]\n",
    "    return concordance\n",
    "\n",
    "def objective(trial):\n",
    "    # Define search space for hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Initialize model with hyperparameters\n",
    "    model = GradientBoostingSurvivalAnalysis(**params)\n",
    "    \n",
    "    def k_fold_cross_validation(model, X, y, k=5):\n",
    "        \"\"\"\n",
    "        Performs k-fold cross-validation for a given model and dataset.\n",
    "\n",
    "        Parameters:\n",
    "            model: The machine learning model to evaluate.\n",
    "            X (numpy.ndarray): The feature matrix.\n",
    "            y (numpy.ndarray): The target vector.\n",
    "            k (int): Number of folds for cross-validation.\n",
    "\n",
    "        Returns:\n",
    "            float: The average accuracy across all folds.\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        fold_size = n // k\n",
    "        scores = []\n",
    "\n",
    "        for i in range(k):\n",
    "            # Splitting data into training and validation sets\n",
    "            validation_X = X[i * fold_size: (i + 1) * fold_size]\n",
    "            validation_y = y[i * fold_size: (i + 1) * fold_size]\n",
    "            train_X = np.concatenate([X[:i * fold_size], X[(i + 1) * fold_size:]])\n",
    "            train_y = np.concatenate([y[:i * fold_size], y[(i + 1) * fold_size:]])\n",
    "\n",
    "            # Fitting the model\n",
    "            model.fit(train_X, train_y)\n",
    "\n",
    "            # Making predictions on the validation set\n",
    "            y_pred = model.predict(validation_X)\n",
    "\n",
    "            # Calculating accuracy\n",
    "            score = scorer(model, validation_X, validation_y)\n",
    "            scores.append(score)\n",
    "\n",
    "        # Returning the average accuracy\n",
    "        return sum(scores) / k\n",
    "    \n",
    "    return k_fold_cross_validation(model, X_train, y_train, k=5)\n",
    "\n",
    "# Create Optuna study object\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Access best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters optimization with a gridsearch approach, we use a gridsearch suggested by the optuna optimization\n",
    "\n",
    "# Define the gridsearch space\n",
    "param_grid = {'n_estimators': [231, 232, 233, 234, 235],\n",
    "              'learning_rate': [0.051, 0.054, 0.06, 0.05774248],\n",
    "              'subsample': [0.62,0.631222,0.64],\n",
    "              'max_depth': [3, 4, 2],\n",
    "              'min_samples_split': [19, 20, 21],\n",
    "              'min_samples_leaf': [11, 12, 13]}\n",
    "\n",
    "param_comb = list(product(*(param_grid[param] for param in param_grid.keys())))\n",
    "\n",
    "# Initialize the model\n",
    "best_score = 0\n",
    "best_params = None\n",
    "\n",
    "for comb in param_comb : \n",
    "    # Create model instance with current hyperparameters\n",
    "    model = GradientBoostingSurvivalAnalysis(n_estimators=comb[0], learning_rate=comb[1], subsample=comb[2], max_depth=comb[3], min_samples_split = comb[4], min_samples_leaf = comb[5], random_state = 42)\n",
    "    \n",
    "    def k_fold_cross_validation(model, X, y, k=5):\n",
    "        \"\"\"\n",
    "        Performs k-fold cross-validation for a given model and dataset.\n",
    "\n",
    "        Parameters:\n",
    "            model: The machine learning model to evaluate.\n",
    "            X (numpy.ndarray): The feature matrix.\n",
    "            y (numpy.ndarray): The target vector.\n",
    "            k (int): Number of folds for cross-validation.\n",
    "\n",
    "        Returns:\n",
    "            float: The average accuracy across all folds.\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        fold_size = n // k\n",
    "        scores = []\n",
    "\n",
    "        for i in range(k):\n",
    "            # Splitting data into training and validation sets\n",
    "            validation_X = X[i * fold_size: (i + 1) * fold_size]\n",
    "            validation_y = y[i * fold_size: (i + 1) * fold_size]\n",
    "            train_X = np.concatenate([X[:i * fold_size], X[(i + 1) * fold_size:]])\n",
    "            train_y = np.concatenate([y[:i * fold_size], y[(i + 1) * fold_size:]])\n",
    "\n",
    "            # Fitting the model\n",
    "            model.fit(train_X, train_y)\n",
    "\n",
    "            # Making predictions on the validation set\n",
    "            y_pred = model.predict(validation_X)\n",
    "\n",
    "            # Calculating accuracy\n",
    "            score = scorer(model, validation_X, validation_y)\n",
    "            scores.append(score)\n",
    "            \n",
    "        # Returning the average accuracy\n",
    "        return sum(scores) / k\n",
    "    \n",
    "    if k_fold_cross_validation(model, X_train, y_train, k=5)>best_score:\n",
    "        best_score = k_fold_cross_validation(model, X_train, y_train, k=5)\n",
    "        best_params = {'n_estimators':comb[0], 'learning_rate':comb[1], 'subsample':comb[2], 'max_depth':comb[3], 'min_samples_split' : comb[4], 'min_samples_leaf': comb[5], 'random_state' : 42}\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best GradientBoostingSurvivalAnalysis model\n",
    "\n",
    "best_SGB = GradientBoostingSurvivalAnalysis(**best_params)\n",
    "best_SGB.fit(X_train, y_train)\n",
    "\n",
    "# Compute the concordance index\n",
    "\n",
    "print(f\"c_index : {concordance_censored(best_SGB,X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising hyperparameters according to metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "cv_param_grid = {\n",
    "    \"estimator__max_depth\": np.arange(1, 10, dtype=int),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_cindex = GridSearchCV(\n",
    "    as_concordance_index_ipcw_scorer(best_SGB, tau=times[-1]),\n",
    "    param_grid=cv_param_grid,\n",
    "    cv=cv,\n",
    "    n_jobs=4,\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_iauc = GridSearchCV(\n",
    "    as_cumulative_dynamic_auc_scorer(best_SGB, times=times),\n",
    "    param_grid=cv_param_grid,\n",
    "    cv=cv,\n",
    "    n_jobs=4,\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_ibs = GridSearchCV(\n",
    "    as_integrated_brier_score_scorer(best_SGB, times=times),\n",
    "    param_grid=cv_param_grid,\n",
    "    cv=cv,\n",
    "    n_jobs=4,\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search_results(gcv, ax, name):\n",
    "    ax.errorbar(\n",
    "        x=gcv.cv_results_[\"param_estimator__max_depth\"].filled(),\n",
    "        y=gcv.cv_results_[\"mean_test_score\"],\n",
    "        yerr=gcv.cv_results_[\"std_test_score\"],\n",
    "    )\n",
    "    ax.plot(\n",
    "        gcv.best_params_[\"estimator__max_depth\"],\n",
    "        gcv.best_score_,\n",
    "        \"ro\",\n",
    "    )\n",
    "    ax.set_ylabel(name)\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "\n",
    "_, axs = plt.subplots(3, 1, figsize=(6, 6), sharex=True)\n",
    "axs[-1].set_xlabel(\"max_depth\")\n",
    "\n",
    "plot_grid_search_results(gcv_cindex, axs[0], \"c-index\")\n",
    "plot_grid_search_results(gcv_iauc, axs[1], \"iAUC\")\n",
    "plot_grid_search_results(gcv_ibs, axs[2], \"$-$IBS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Interpretability methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = best_SGB.feature_importances_\n",
    "\n",
    "total_importance = np.sum(feature_importances)\n",
    "importances_percentage = (feature_importances / total_importance) * 100\n",
    "importances_percentage.sort()\n",
    "variable_names = X_train.columns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importances_percentage)), importances_percentage, tick_label=variable_names)\n",
    "plt.xlabel('Importance en pourcentage')\n",
    "plt.ylabel('Variables')\n",
    "plt.title('Importance des variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_gb = SurvLimeExplainer(\n",
    "    training_features=X_train,\n",
    "    training_events=[tp[0] for tp in y_train],\n",
    "    training_times=[tp[1] for tp in y_train],\n",
    "    model_output_times=best_SGB.event_times_,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.array([0.1,0.1])\n",
    "\n",
    "b_gb = explainer_gb.explain_instance(\n",
    "    data_row=x_new,\n",
    "    predict_fn=best_SGB.predict_cumulative_hazard_function,\n",
    "    num_samples=1000,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "explainer_gb.plot_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use shap method to interpret this model, since the output of a SSVM is a risk score, we can use classical shap method to interpret it\n",
    "\n",
    "# use Tree Explainer SHAP to explain test set predictions\n",
    "explainer = shap.Explainer(best_SGB.predict, X_train)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values,X_test)\n",
    "\n",
    "shap.dependence_plot(\"enum\", shap_values, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
